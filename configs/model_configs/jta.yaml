# =============================================================================
# JTA Dataset Experiment Configuration
# =============================================================================
# Joint Track Auto (JTA) - Synthetic dataset with native 3D pose annotations
# Best dataset for training pose-guided trajectory prediction
# 10M+ pose annotations, 22 body joints, occlusion flags
# =============================================================================

experiment:
  name: "jta_pose_trajectory"
  description: "Pose-guided trajectory prediction on JTA dataset"
  
# =============================================================================
# Dataset Configuration
# =============================================================================
dataset:
  name: "jta"
  data_root: "data/raw/jta"
  processed_root: "data/processed/jta"
  
  # JTA structure
  structure:
    annotations: "annotations"          # JSON annotation files
    videos: "videos"                     # Video files (optional)
    train: "train"
    val: "val"
    test: "test"
    
  # Sequence configuration
  sequences:
    train_count: 256                     # Number of training sequences
    val_count: 128                       # Number of validation sequences
    test_count: 128                      # Number of test sequences
    
  # Original video properties
  video:
    resolution: [1920, 1080]             # HD resolution
    original_fps: 30                     # Original frame rate
    
  # Downsampling for trajectory prediction
  sampling:
    target_fps: 2.5                      # Standard trajectory prediction FPS
    frame_skip: 12                       # 30 / 2.5 = 12
    
# =============================================================================
# Pose Configuration (JTA-specific)
# =============================================================================
pose:
  # JTA uses 22 joints
  num_joints: 22
  
  # Joint names (JTA format - OFFICIAL from dataset documentation)
  joints:
    0: "head_top"
    1: "head_center"
    2: "neck"
    3: "right_clavicle"
    4: "right_shoulder"
    5: "right_elbow"
    6: "right_wrist"
    7: "left_clavicle"
    8: "left_shoulder"
    9: "left_elbow"
    10: "left_wrist"
    11: "spine0"
    12: "spine1"
    13: "spine2"                         # Middle spine - use as pelvis/center
    14: "spine3"
    15: "spine4"
    16: "right_hip"
    17: "right_knee"
    18: "right_ankle"
    19: "left_hip"
    20: "left_knee"
    21: "left_ankle"
    
  # Key joints for trajectory prediction (CORRECTED indices)
  key_joints:
    pelvis: 13                           # spine2 - Center for normalization
    ankles: [18, 21]                     # right_ankle, left_ankle - Gait phase
    knees: [17, 20]                      # right_knee, left_knee - Speed estimation
    hips: [16, 19]                       # right_hip, left_hip - Direction change
    shoulders: [4, 8]                    # right_shoulder, left_shoulder - Turn intention
    wrists: [6, 10]                      # right_wrist, left_wrist - Arm swing
    elbows: [5, 9]                       # right_elbow, left_elbow
    neck: 2                              # neck
    
  # Coordinate system
  coordinates: "3d"                      # x, y, z in world coordinates (meters)
  
  # Annotation format (10 columns per row)
  annotation_format:
    frame_number: 0
    person_id: 1
    joint_type: 2
    x2D: 3                               # Pixels
    y2D: 4                               # Pixels
    x3D: 5                               # Meters
    y3D: 6                               # Meters
    z3D: 7                               # Meters
    occluded: 8
    self_occluded: 9
    
  # Camera intrinsics (same for all JTA sequences)
  camera:
    fx: 1158
    fy: 1158
    cx: 960
    cy: 540
    resolution: [1920, 1080]
  
  # Normalization
  normalization:
    center_joint: 13                     # spine2 (pelvis)
    scale_by_torso: true                 # Normalize by torso length
    torso_joints: [2, 13]                # Neck to spine2 (pelvis)
    
# =============================================================================
# Sequence Configuration
# =============================================================================
sequence:
  # Standard protocol
  obs_len: 8                             # 3.2 seconds observation
  pred_len: 12                           # 4.8 seconds prediction
  fps: 2.5
  
  # Sequence creation
  skip: 1
  min_ped: 1
  max_ped: 20                            # JTA can have many pedestrians
  
  # Filtering
  min_trajectory_length: 20              # Minimum frames for valid trajectory
  occlusion_threshold: 0.5               # Max occlusion ratio to include
  
# =============================================================================
# Model Configuration
# =============================================================================
model:
  config: "configs/model_configs/social_pose.yaml"
  
  # Model overrides for JTA (native pose available)
  overrides:
    pose_encoder:
      enabled: true                      # ✅ Use native 3D pose
      num_joints: 22                     # JTA has 22 joints
      input_dim: 3                       # 3D coordinates
      
    velocity_encoder:
      enabled: true
      
    social_pooling:
      enabled: true
      neighborhood_size: 3.0             # Larger neighborhood for JTA
      
# =============================================================================
# Training Configuration
# =============================================================================
training:
  # Basic settings
  batch_size: 64
  num_epochs: 300                        # More epochs for larger dataset
  num_workers: 8                         # More workers for JTA
  
  # Optimizer (transformer-friendly)
  optimizer:
    name: "adamw"
    lr: 0.00075                          # 7.5e-4 for transformer
    weight_decay: 0.0005
    betas: [0.9, 0.98]
    
  # Scheduler
  scheduler:
    name: "cosine"
    warmup_epochs: 10
    min_lr: 0.00001
    
  # Loss
  loss:
    lambda_ade: 1.0
    lambda_fde: 1.0
    lambda_speed: 0.5                    # Speed consistency loss
    lambda_diversity: 0.5
    k_samples: 20
    
  # Regularization
  grad_clip: 1.0
  label_smoothing: 0.0
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 30
    metric: "val_ade"
    
  # Checkpointing
  save_dir: "checkpoints/trained/jta"
  save_every: 10
  save_best: true
  
  # Mixed precision (for efficiency)
  mixed_precision: true
  
# =============================================================================
# Data Augmentation
# =============================================================================
augmentation:
  enabled: true
  
  # Spatial augmentation
  rotation:
    enabled: true
    range: [-180, 180]
    
  scaling:
    enabled: true
    range: [0.8, 1.2]
    
  translation:
    enabled: true
    range: [-0.5, 0.5]                   # Meters
    
  # Noise
  noise:
    trajectory:
      enabled: true
      std: 0.02                          # 2cm noise on trajectory
    pose:
      enabled: true
      std: 0.01                          # 1cm noise on pose joints
      
  # Speed augmentation (critical for adaptive velocity)
  speed_augment:
    enabled: true
    range: [0.5, 2.0]
    
  # Flip
  flip:
    enabled: true
    prob: 0.5
    # Joint mapping for flipping (left <-> right)
    joint_flip_pairs: [[3,6], [4,7], [5,8], [11,14], [12,15], [13,16], [17,18], [19,20]]
    
  # Pose-specific augmentation
  pose_dropout:
    enabled: true
    prob: 0.1                            # Randomly drop joints (occlusion simulation)
    min_visible: 10                      # Minimum visible joints
    
# =============================================================================
# Evaluation Configuration
# =============================================================================
evaluation:
  k_samples: 20
  
  metrics:
    - "ade"
    - "fde"
    - "ade_at_time"                      # ADE at specific horizons
    - "collision_rate"
    - "pose_consistency"                 # Pose-trajectory alignment
    
  time_horizons: [1.0, 2.0, 3.0, 4.8]   # Seconds
  
  # Visualization
  visualize:
    enabled: true
    num_samples: 20
    save_dir: "outputs/visualizations/jta"
    show_skeleton: true
    show_trajectory: true
    show_prediction: true
    
# =============================================================================
# Data Split
# =============================================================================
split:
  train: 0.7
  val: 0.15
  test: 0.15
  
  # Or use predefined splits
  use_predefined: true
  predefined:
    train: "train"
    val: "val"
    test: "test"
    
# =============================================================================
# Expected Results (JTA benchmark)
# =============================================================================
# Without pose (trajectory only):
# | Metric | Value |
# |--------|-------|
# | ADE    | 0.52  |
# | FDE    | 1.10  |
#
# With pose (Social-Pose style):
# | Metric | Value | Improvement |
# |--------|-------|-------------|
# | ADE    | 0.39  | +25%        |
# | FDE    | 0.78  | +29%        |
#
# Key insight: Pose helps most on turning trajectories (+37%)
# =============================================================================

# =============================================================================
# JTA Dataset Download Instructions
# =============================================================================
# 1. Request access at: https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=25
# 2. Download annotations (~5GB) and optionally videos (~15GB)
# 3. Extract to data/raw/jta/
# 4. Run preprocessing: python scripts/preprocess_jta.py
#
# Expected structure:
# data/raw/jta/
# ├── annotations/
# │   ├── train/
# │   │   ├── seq_000001.json
# │   │   └── ...
# │   ├── val/
# │   └── test/
# └── videos/ (optional)
#     ├── train/
#     └── val/
# =============================================================================

# =============================================================================
# Run Commands
# =============================================================================
# Preprocess:
#   python scripts/preprocess_jta.py --config configs/experiment_configs/jta.yaml
#
# Train:
#   python experiments/train.py --config configs/experiment_configs/jta.yaml
#
# Evaluate:
#   python experiments/evaluate.py --config configs/experiment_configs/jta.yaml
# =============================================================================
